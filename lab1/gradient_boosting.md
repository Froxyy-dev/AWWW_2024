



# Gradient Boosting

## Description


Gradient boosting is a powerful machine learning algorithm that combines multiple “weak learners” into a single predictive model. For example, it could be used to combine the output of several decision trees or to combine multiple support vector machines. The result is an algorithm that often outperforms the random forest and other machine learning algorithms in terms of predictive performance.

## Duck response

### Gradient boosting - Wikipedia [link to wikipedia](https://en.wikipedia.org/wiki/Gradient_boosting)

### Wikipedia description


Gradient boosting is a machine learning technique based on boosting in a functional space, where the target is pseudo-residuals rather than the typical residuals used in traditional boosting. It gives a prediction model in the form of an ensemble of weak prediction models, i.e., models that make very few assumptions about the data, which are typically simple decision trees. When a decision tree is the weak learner, the resulting algorithm is called gradient-boosted trees; it usually outperforms random forest. A gradient-boosted trees model is built in a stage-wise fashion as in other boosting methods, but it generalizes the other methods by allowing optimization of an arbitrary differentiable loss function.

### Wikipedia image


![Image: ](https://tse1.mm.bing.net/th?id=OIP.9a3kNgf3YJxXPtIErVRXjgHaHa&pid=Api)